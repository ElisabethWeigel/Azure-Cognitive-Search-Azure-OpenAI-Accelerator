{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Azure AI Search integrated vectorization sample\n",
        "This code demonstrates how to use Azure AI Search as a vector store by automatically chunking and generating embeddings using the AzureOpenAIEmbedding skill as part of the skillset pipeline in Azure AI Search. \n",
        "## Prerequisites\n",
        "To run the code, install the following packages. This sample currently uses version `11.4.0b12`. Please note, that integrated vectorization feature is in preview and has not been published to [azure-search-documents](https://pypi.org/project/azure-search-documents/#description) on pypi. If you'd like to use this feature, please reference the whl file. We hope to publish an updated version soon!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install whl/azure_search_documents-11.4.0b12-py3-none-any.whl  --quiet\n",
        "! pip install openai azure-storage-blob python-dotenv --quiet"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Import required libraries and environment variables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries  \n",
        "from azure.core.credentials import AzureKeyCredential  \n",
        "from azure.search.documents import SearchClient  \n",
        "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
        "from azure.search.documents.models import (\n",
        "    QueryAnswerType,\n",
        "    QueryCaptionType,\n",
        "    QueryLanguage,\n",
        "    QueryType,\n",
        "    RawVectorQuery,\n",
        "    VectorizableTextQuery,\n",
        "    VectorFilterMode,    \n",
        ")\n",
        "from azure.search.documents.indexes.models import (  \n",
        "    AzureOpenAIEmbeddingSkill,  \n",
        "    AzureOpenAIParameters,  \n",
        "    AzureOpenAIVectorizer,  \n",
        "    ExhaustiveKnnParameters,  \n",
        "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
        "    FieldMapping,  \n",
        "    HnswParameters,  \n",
        "    HnswVectorSearchAlgorithmConfiguration,  \n",
        "    IndexProjectionMode,  \n",
        "    InputFieldMappingEntry,  \n",
        "    OutputFieldMappingEntry,  \n",
        "    PrioritizedFields,    \n",
        "    SearchField,  \n",
        "    SearchFieldDataType,  \n",
        "    SearchIndex,  \n",
        "    SearchIndexer,  \n",
        "    SearchIndexerDataContainer,  \n",
        "    SearchIndexerDataSourceConnection,  \n",
        "    SearchIndexerIndexProjectionSelector,  \n",
        "    SearchIndexerIndexProjections,  \n",
        "    SearchIndexerIndexProjectionsParameters,  \n",
        "    SearchIndexerSkillset,  \n",
        "    SemanticConfiguration,  \n",
        "    SemanticField,  \n",
        "    SemanticSettings,  \n",
        "    SplitSkill,  \n",
        "    VectorSearch,  \n",
        "    VectorSearchAlgorithmKind,  \n",
        "    VectorSearchAlgorithmMetric,  \n",
        "    VectorSearchProfile,  \n",
        ")  \n",
        "\n",
        "from azure.storage.blob import BlobServiceClient  \n",
        "import openai  \n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1701869989595
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"cogsrch-index-files\"\n",
        "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
        "BLOB_CONTAINER_NAME = \"demo-vbd-mercedes\"\n",
        "\n",
        "#AZURE_SEARCH_SERVICE_ENDPOINT=YOUR-SEARCH-SERVICE-ENDPOINT\n",
        "#AZURE_SEARCH_INDEX_NAME=YOUR-SEARCH-SERVICE-INDEX-NAME\n",
        "#AZURE_SEARCH_ADMIN_KEY=YOUR-SEARCH-SERVICE-ADMIN-KEY"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701869991676
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv  \n",
        "import os  \n",
        "  \n",
        "# Configure environment variables  \n",
        "load_dotenv(\"credentials.env\")\n",
        "service_endpoint =os.getenv(\"AZURE_SEARCH_ENDPOINT\")  #os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
        "index_name =\"cogsrch-index-files\" # os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
        "key = os.getenv(\"AZURE_SEARCH_KEY\")  #remose _ADMIN_ here\n",
        "openai.api_type = \"azure\"  \n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
        "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
        "model: str = \"text-embedding-ada-002\"  \n",
        "blob_connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")  \n",
        "container_name =\"demo-vbd-mercedes\" # os.getenv(\"BLOB_CONTAINER_NAME\")  \n",
        "credential = AzureKeyCredential(key)  "
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701869999767
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Blob Storage  \n",
        "Retrieve documents from Blob Storage. You can use the sample documents in the [documents](../data/documents) folder.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Blob Storage\n",
        "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "blobs = container_client.list_blobs()\n",
        "\n",
        "first_blob = next(blobs)\n",
        "blob_url = container_client.get_blob_client(first_blob).url\n",
        "print(f\"URL of the first blob: {blob_url}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "URL of the first blob: https://storagedemoopenai.blob.core.windows.net/demo-vbd-mercedes/2602036007%20%5BOriginal%20en%5D%20Canada%20Federal%20Gaz%20I%202011-02-26%20mercury.pdf.pdf\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1701870003099
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{index_name}-blob\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cogsrch-index-files-blob\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701870005822
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect your Blob storage to a data source in Azure AI Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data source \n",
        "ds_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))\n",
        "container = SearchIndexerDataContainer(name=container_name)\n",
        "data_source_connection = SearchIndexerDataSourceConnection(\n",
        "    name=f\"{index_name}-blob\",\n",
        "    type=\"azureblob\",\n",
        "    connection_string=blob_connection_string,\n",
        "    container=container\n",
        ")\n",
        "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
        "\n",
        "print(f\"Data source '{data_source.name}' created or updated\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data source 'cogsrch-index-files-blob' created or updated\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1701870008863
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a search index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a search index  \n",
        "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)  \n",
        "fields = [  \n",
        "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
        "    SearchField(name=\"title\", type=SearchFieldDataType.String),  \n",
        "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
        "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
        "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile=\"myHnswProfile\"),\n",
        "    SearchField(name=\"filter\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),    \n",
        "]  \n",
        "  \n",
        "# Configure the vector search configuration  \n",
        "vector_search = VectorSearch(  \n",
        "    algorithms=[  \n",
        "        HnswVectorSearchAlgorithmConfiguration(  \n",
        "            name=\"myHnsw\",  \n",
        "            kind=VectorSearchAlgorithmKind.HNSW,  \n",
        "            parameters=HnswParameters(  \n",
        "                m=4,  \n",
        "                ef_construction=400,  \n",
        "                ef_search=500,  \n",
        "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
        "            ),  \n",
        "        ),  \n",
        "        ExhaustiveKnnVectorSearchAlgorithmConfiguration(  \n",
        "            name=\"myExhaustiveKnn\",  \n",
        "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,  \n",
        "            parameters=ExhaustiveKnnParameters(  \n",
        "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
        "            ),  \n",
        "        ),  \n",
        "    ],  \n",
        "    profiles=[  \n",
        "        VectorSearchProfile(  \n",
        "            name=\"myHnswProfile\",  \n",
        "            algorithm=\"myHnsw\",  \n",
        "            vectorizer=\"myOpenAI\",  \n",
        "        ),  \n",
        "        VectorSearchProfile(  \n",
        "            name=\"myExhaustiveKnnProfile\",  \n",
        "            algorithm=\"myExhaustiveKnn\",  \n",
        "            vectorizer=\"myOpenAI\",  \n",
        "        ),  \n",
        "    ],  \n",
        "    vectorizers=[  \n",
        "        AzureOpenAIVectorizer(  \n",
        "            name=\"myOpenAI\",  \n",
        "            kind=\"azureOpenAI\",  \n",
        "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
        "                resource_uri=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
        "                deployment_id=model,  \n",
        "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
        "            ),  \n",
        "        ),  \n",
        "    ],  \n",
        ")  \n",
        "  \n",
        "semantic_config = SemanticConfiguration(  \n",
        "    name=\"my-semantic-config\",  \n",
        "    prioritized_fields=PrioritizedFields(  \n",
        "        prioritized_content_fields=[SemanticField(field_name=\"chunk\")]  \n",
        "    ),  \n",
        ")  \n",
        "  \n",
        "# Create the semantic settings with the configuration  \n",
        "semantic_settings = SemanticSettings(configurations=[semantic_config])  \n",
        "  \n",
        "# Create the search index with the semantic settings  \n",
        "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_settings=semantic_settings)  \n",
        "result = index_client.create_or_update_index(index)  \n",
        "print(f\"{result.name} created\")  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cogsrch-index-files created\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1701870012598
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a skillset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a skillset  \n",
        "skillset_name = f\"{index_name}-skillset\"  \n",
        "  \n",
        "split_skill = SplitSkill(  \n",
        "    description=\"Split skill to chunk documents\",  \n",
        "    text_split_mode=\"pages\",  \n",
        "    context=\"/document\",  \n",
        "    maximum_page_length=2048,  \n",
        "    page_overlap_length=20,  \n",
        "    inputs=[  \n",
        "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
        "    ],  \n",
        "    outputs=[  \n",
        "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
        "    ],  \n",
        ")  \n",
        "  \n",
        "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
        "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
        "    context=\"/document/pages/*\",  \n",
        "    resource_uri=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
        "    deployment_id=model,  \n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
        "    inputs=[  \n",
        "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
        "    ],  \n",
        "    outputs=[  \n",
        "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
        "    ],  \n",
        ")  \n",
        "  \n",
        "index_projections = SearchIndexerIndexProjections(  \n",
        "    selectors=[  \n",
        "        SearchIndexerIndexProjectionSelector(  \n",
        "            target_index_name=index_name,  \n",
        "            parent_key_field_name=\"parent_id\",  \n",
        "            source_context=\"/document/pages/*\",  \n",
        "            mappings=[  \n",
        "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
        "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),  \n",
        "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
        "            ],  \n",
        "        ),  \n",
        "    ],  \n",
        "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
        "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
        "    ),  \n",
        ")  \n",
        "  \n",
        "skillset = SearchIndexerSkillset(  \n",
        "    name=skillset_name,  \n",
        "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
        "    skills=[split_skill, embedding_skill],  \n",
        "    index_projections=index_projections,  \n",
        ")  \n",
        "  \n",
        "client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
        "client.create_or_update_skillset(skillset)  \n",
        "print(f\"{skillset.name} created\")  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cogsrch-index-files-skillset created\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1701870018006
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an indexer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer  \n",
        "indexer_name = f\"{index_name}-indexer\"  \n",
        "  \n",
        "indexer = SearchIndexer(  \n",
        "    name=indexer_name,  \n",
        "    description=\"Indexer to index documents and generate embeddings\",  \n",
        "    skillset_name=skillset_name,  \n",
        "    target_index_name=index_name,  \n",
        "    data_source_name=data_source.name,  \n",
        "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
        "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")]  \n",
        ")  \n",
        "  \n",
        "indexer_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
        "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
        "  \n",
        "# Run the indexer  \n",
        "indexer_client.run_indexer(indexer_name)  \n",
        "print(f' {indexer_name} created')  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " cogsrch-index-files-indexer created\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1701870038731
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Perform a vector similarity search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pure Vector Search\n",
        "query = \"Welche Regeln gelten für Motoren bei Land Transporten?\"  \n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "vector_query = VectorizableTextQuery(text=query, k=2, fields=\"vector\", exhaustive=True)\n",
        "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
        "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k=3, fields=\"vector\")\n",
        "  \n",
        "results = search_client.search(  \n",
        "    search_text=None,  \n",
        "    vector_queries= [vector_query],\n",
        "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
        "    #filter=\"filter eq 'id1' \",\n",
        "    #filter=\"filter/any(filter: search.in(filter, 'group_id1, group_id2'))\"  ,\n",
        "    top=1\n",
        ")  \n",
        "  \n",
        "for result in results:  \n",
        "    print(f\"parent_id: {result['parent_id']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    print(f\"Content: {result['chunk']}\")   \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "parent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_0\nScore: 0.80743897\nContent: Transport Canada Transports Canada \nSafety and Security Sécurité et sûreté \n\n \nRoad Safety  Sécurité routière \n \n \n\nStandards and Regulations Division \n \n\n \n\n \n\nTECHNICAL STANDARDS DOCUMENT \n\nNo. 206, Revision 2 \n\n \n\n \n\nDoor Locks and Door Retention \nComponents \n\n \n \n\nThe text of this document is based on Federal Motor \nVehicle Safety Standard No. 206, Door Locks and Door \n\nRetention Components, as published in the Federal \nRegister on February 19, 2010 (Vol. 75, No. 33, p. 7370). \n\n \n \n \n\nEffective Date:  March 11, 2008 \nMandatory Compliance Date: September 1, 20111 \n\n \n\n \n\nStandards Research and Development Branch \nRoad Safety and Motor Vehicle Regulation Directorate \n\nTRANSPORT CANADA \nOttawa, Ontario \n\nK1A 0N5 \n\n                     \n1 Note: The previous mandatory compliance date was September 1, 2009. \n\nhkl\nLinien\n\nhkl\nLinien\n\nhkl\nLinien\n\nhttp://cis-gso.daimler.com/servlet/GSOUser/docalias_content.en/node.0/c0bdkjfla/?docalias=3444.pdf\n\n\n \n\n \n\nTechnical Standards Document \nNumber 206, Revision 2 \n\n \nDoor Locks and Door Retention Components \n\n(Ce document est aussi disponible en français.) \n\nIntroduction \n\nAs defined by section 12 of the Motor Vehicle Safety Act, a Technical Standards Document \n(TSD) is a document that reproduces an enactment of a foreign government (e.g. a Federal \nMotor Vehicle Safety Standard issued by the U.S. National Highway Traffic Safety \nAdministration).  According to the Act, the Motor Vehicle Safety Regulations may alter or \noverride some provisions contained in a TSD or specify additional requirements; \nconsequently, it is advisable to read a TSD in conjunction with the Act and its counterpart \nRegulation.  As a guide, where the corresponding Regulation contains additional \nrequirements, footnotes indicate the amending subsection number. \n \nTSDs are revised from time to time in order to incorporate amendments made to the \nreference document, at which time a Notice of Revision is published in the Canada Gazette, \nPart I.\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1701870125285
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform a hybrid search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid Search\n",
        "query = \"Welche Regeln gelten für Motoren bei Land Transporten?\"  \n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"vector\", exhaustive=True)\n",
        "  \n",
        "results = search_client.search(  \n",
        "    search_text=query,  \n",
        "    vector_queries= [vector_query],\n",
        "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
        "    top=3\n",
        ")  \n",
        "  \n",
        "for result in results:  \n",
        "    print(f\"parent_id: {result['parent_id']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    #print(f\"Content: {result['chunk']}\")  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "parent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2\nchunk_id: 77e6ceb46773_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2_pages_45\nScore: 0.01666666753590107\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_0\nScore: 0.01666666753590107\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2\nchunk_id: 77e6ceb46773_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2_pages_44\nScore: 0.016393441706895828\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1701870148480
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform a hybrid search + semantic reranking"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Hybrid Search\n",
        "query = \"Welche Regeln gelten für Motoren bei Land Transporten?\"\n",
        "\n",
        "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
        "vector_query = VectorizableTextQuery(text=query, k=2, fields=\"vector\", exhaustive=True)\n",
        "\n",
        "results = search_client.search(  \n",
        "    search_text=query,\n",
        "    vector_queries=[vector_query],\n",
        "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
        "    query_type=QueryType.SEMANTIC, query_language=QueryLanguage.EN_US, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
        "    top=2\n",
        ")\n",
        "\n",
        "semantic_answers = results.get_answers()\n",
        "for answer in semantic_answers:\n",
        "    if answer.highlights:\n",
        "        print(f\"Semantic Answer: {answer.highlights}\")\n",
        "    else:\n",
        "        print(f\"Semantic Answer: {answer.text}\")\n",
        "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
        "\n",
        "for result in results:\n",
        "    print(f\"parent_id: {result['parent_id']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
        "    #print(f\"Content: {result['chunk']}\")  \n",
        "\n",
        "    captions = result[\"@search.captions\"]\n",
        "    if captions:\n",
        "        caption = captions[0]\n",
        "        if caption.highlights:\n",
        "            print(f\"Caption: {caption.highlights}\\n\")\n",
        "        else:\n",
        "            print(f\"Caption: {caption.text}\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "parent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_0\nScore: 0.01666666753590107\nReranker Score: 1.558976650238037\nCaption: effective date:  march 11, 2008  mandatory compliance date: september 1, 20111         standards research and development branch  road safety and<em> motor vehicle regulation</em> directorate<em>   transport</em> canada  ottawa, ontario   k1a 0n5                         1 note: the previous mandatory compliance date was september 1, 2009.   hkl linien  hkl linien …\n\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2\nchunk_id: 77e6ceb46773_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwMjAzNjAwNyUyMFtPcmlnaW5hbCUyMGVuXSUyMENhbmFkYSUyMEZlZGVyYWwlMjBHYXolMjBJJTIwMjAxMS0wMi0yNiUyMG1lcmN1cnkucGRmLnBkZg2_pages_45\nScore: 0.01666666753590107\nReranker Score: 0.8075979948043823\nCaption: This sum of avoided releases can be broken down further be- tween releases to land (82% — 46 315 kg), to air (18% —  9 991 kg) and water (0.2% — 123 kg). Most of the mercury that  is released to land from products is assumed to go to the landfill  where a small percentage each year will leach to water or emit to  air as landfill gas.\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1701870166083
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}