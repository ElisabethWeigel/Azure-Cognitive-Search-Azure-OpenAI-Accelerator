{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Queries with Azure OpenAI"
      ],
      "metadata": {},
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you have your Search Engine loaded **from two different data sources in two diferent text-based indexes**, on this notebook we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results.\n",
        "\n",
        "The idea is that a user can ask a question about Computer Science (first datasource/index) or about Covid (second datasource/index), and the engine will respond accordingly.\n",
        "This **Multi-Index** demo, mimics the scenario where a company loads multiple type of documents of different types and about completly different topics and the search engine must respond with the most relevant results."
      ],
      "metadata": {},
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables"
      ],
      "metadata": {},
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_PROMPT_TEMPLATE\n",
        "from common.utils import (\n",
        "    get_search_results,\n",
        "    model_tokens_limit,\n",
        "    num_tokens_from_docs,\n",
        "    num_tokens_from_string\n",
        ")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1701870474279
        }
      },
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {},
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Azure OpenAI\n",
        "\n",
        "To use OpenAI to get a better answer to our question, the thought process is simple: let's **give the answer and the content of the documents from the search result to the GPT model as context and let it provide a better response**.\n",
        "\n",
        "Now, before we do this, we need to understand a few things first:\n",
        "\n",
        "1) Chainning and Prompt Engineering\n",
        "2) Embeddings\n",
        "\n",
        "We will use a library call **LangChain** that wraps a lot of boiler plate code.\n",
        "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
      ],
      "metadata": {},
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1701870480355
        }
      },
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note**: Starting now, we will utilize OpenAI models. Please ensure that you have deployed the following models within the Azure OpenAI portal using these precise deployment names:\n",
        "\n",
        "- text-embedding-ada-002\n",
        "- gpt-35-turbo\n",
        "- gpt-35-turbo-16k\n",
        "- gpt-4\n",
        "- gpt-4-32k\n",
        "\n",
        "Should you have deployed the models under different names, the code provided below will not function as expected. To resolve this, you would need to modify the variable names throughout all the notebooks."
      ],
      "metadata": {},
      "id": "325d9138-2250-4f6b-bc88-50d7957f8d33"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A gentle intro to chaining LLMs and prompt engineering"
      ],
      "metadata": {},
      "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains are what you get by connecting one or more large language models (LLMs) in a logical way. (Chains can be built of entities other than LLMs but for now, let’s stick with this definition for simplicity).\n",
        "\n",
        "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
        "\n",
        "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
        "\n",
        "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
        "\n",
        "Here’s an example:"
      ],
      "metadata": {},
      "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4\" # options: gpt-35-turbo, gpt-35-turbo-16k, gpt-4, gpt-4-32k\n",
        "COMPLETION_TOKENS = 1000\n",
        "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=COMPLETION_TOKENS)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1701870493020
        }
      },
      "id": "13df9247-e784-4e04-9475-55e672efea47"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Welche Regeln gelten für Motoren bei Land Transporten?\"\n",
        "\n",
        "# Now we create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"language\"],\n",
        "    template='Answer the following question: \"{question}\". Give your response in {language}',\n",
        ")\n",
        "\n",
        "print(prompt.format(question=QUESTION, language=\"English\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer the following question: \"Welche Regeln gelten für Motoren bei Land Transporten?\". Give your response in English\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1701870591601
        }
      },
      "id": "7b0520b9-83b2-49fd-ad84-624cb0f15ce1"
    },
    {
      "cell_type": "code",
      "source": [
        "# And finnaly we create our first generic chain\n",
        "chain_chat = LLMChain(llm=llm, prompt=prompt)\n",
        "chain_chat({\"question\": QUESTION, \"language\": \"English\"})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "{'question': 'Welche Regeln gelten für Motoren bei Land Transporten?',\n 'language': 'English',\n 'text': 'The question asks: \"What rules apply to engines for land transports?\" \\n\\nThe rules for engines in land transports can vary depending on the country and the type of vehicle. However, some common rules include:\\n\\n1. Emission Standards: Engines must meet certain emission standards to reduce air pollution. These standards can vary by country and vehicle type.\\n\\n2. Maintenance and Inspection: Regular maintenance and inspection of engines are often required to ensure they are operating safely and efficiently.\\n\\n3. Noise Regulations: There may be rules regarding the amount of noise an engine can produce.\\n\\n4. Fuel Efficiency Standards: Some countries have fuel efficiency standards that engines must meet.\\n\\n5. Safety Standards: Engines must meet certain safety standards to prevent accidents and breakdowns.\\n\\n6. Engine Size/Power: Some types of vehicles may have restrictions on engine size or power.\\n\\nRemember to check the specific rules and regulations in your own country or region.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1701870611384
        }
      },
      "id": "dcc7dae3-6b88-4ea6-be43-b178ebc559dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: this is the first time you use OpenAI in this Accelerator, so if you get a Resource not found error, is most likely because the name of your OpenAI model deployment is different than the variable MODEL set above"
      ],
      "metadata": {},
      "id": "cd8539d0-a538-4368-82c3-5f91d8370f1e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
        "\n",
        "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
      ],
      "metadata": {},
      "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The second type of Chains are Utility:**\n",
        "\n",
        "* Utility — These are specialized chains, comprised of many LLMs to help solve a specific task. For example, LangChain supports some end-to-end chains (such as [QA_WITH_SOURCES](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) for QnA Doc retrieval, Summarization, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). \n",
        "\n",
        "We will look at one specific chain called **qa_with_sources** in this workshop for digging deeper and solve our use case of enhancing the results of Azure Cognitive Search."
      ],
      "metadata": {
        "tags": []
      },
      "id": "12c48038-b1af-4228-8ffb-720e554fd3b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "But before dealing with the utility chain needed, we need to deal first with this problem: **the content of the search result files is or can be very lengthy, more than the allowed tokens allowed by the GPT Azure OpenAI models**. \n",
        "\n",
        "This is where the concept of embeddings/vectors come into place.\n",
        "\n",
        "## Embeddings and Vector Search\n",
        "\n",
        "From the Azure OpenAI documentation ([HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=python)), An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar. \n",
        "\n",
        "To address the challenge of accommodating context within the token limit of a Language Model (LLM), the solution involves the following steps:\n",
        "\n",
        "1. **Segmenting Documents**: Divide the documents into smaller segments or chunks.\n",
        "2. **Vectorization of Chunks**: Transform these chunks into vectors using appropriate techniques.\n",
        "3. **Vector Semantic Search**: Execute a semantic search using vectors to identify the top chunks similar to the given question.\n",
        "4. **Optimal Context Provision**: Provide the LLM with the most relevant and concise context, thereby achieving an optimal balance between comprehensiveness and lengthiness.\n",
        "\n",
        "\n",
        "Notice that **the documents chunks are already done in Azure Search**. *ordered_content* dictionary (created a few cells above) contains the chunks of each document. So we don't really need to chunk them again, but we still need to make sure that we can be as fast as possible and that we are below the max allowed input token limits of our selected OpenAI model."
      ],
      "metadata": {},
      "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our ultimate goal is to rely solely on vector indexes. While it is possible to manually code parsers with OCR for various file types and develop a scheduler to synchronize data with the index, there is a more efficient alternative: **Azure Cognitive Search is soon going to release automated chunking strategies and vectorization within the next months**, so we have three options: \n",
        "1. Wait for this functionality while in the meantime manually push chunks and its vectors to the vector-based indexes \n",
        "2. Fill up the vector-based indexes on-demand, as documents are discovered by users\n",
        "3. Use custom skills (for chunking and vectorization) and use knowledge stores in order to create a vector-base index from a text-based-ai-enriched index at ingestion time. See [HERE](https://github.com/Azure/cognitive-search-vector-pr/blob/main/demo-python/code/azure-search-vector-ingestion-python-sample.ipynb) for instructions on how to do this.\n",
        "\n",
        "In this notebook we are going to implement Option 2: **Create vector-based indexes per each text-based indexes and fill them up on-demand as documents are discovered**. Why? because is simpler and quick to implement, while we wait for Option 1 to become a feature of Azure Search Engine (which is the automation of Option 3 inside the search engine).\n",
        "\n",
        "As observed in Notebooks 1 and 2, each text-based index contains a field named `vectorized` that we have not utilized yet. We will now harness this field. The objective is to avoid vectorizing all documents at the time of ingestion (Option 3). Instead, we can vectorize the chunks as users search for or discover documents. This approach ensures that we allocate funds and resources only when the documents are actually required. Typically, in an organization with a vast repository of documents in a data lake, only 20% of the documents are frequently accessed, while the rest remain untouched. This phenomenon mirrors the [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle) found in nature."
      ],
      "metadata": {},
      "id": "80e79235-3d8b-4713-9336-5004cc4a1556"
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"cogsrch-index-files\"\n",
        "index2_name = \"cogsrch-index-csv\"\n",
        "indexes = [index_name, index2_name]"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {},
      "id": "12682a1b-df92-49ce-a638-7277103f6cb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not duplicate code, we have put many of the code used above into functions. These functions are in the `common/utils.py` and `common/prompts.py` files. This way we can use these functios in the app that we will build later."
      ],
      "metadata": {},
      "id": "78a6d6a7-18ef-45b2-a216-3c1f50006593"
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1701870924454
        }
      },
      "id": "2937ba3b-098d-43f8-8498-3534882a5cc7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For vector search is not recommended to give more than k=5 chunks (of max 5000 characters each) to the LLM as context. Otherwise you can have issues later with the token limit trying to have a conversation with memory."
      ],
      "metadata": {},
      "id": "1a98a974-0633-499f-a8f0-29bf6242e737"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate number of tokens of our docs\n",
        "if(len(top_docs)>0):\n",
        "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
        "    prompt_tokens = num_tokens_from_string(COMBINE_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
        "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
        "    \n",
        "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
        "    \n",
        "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
        "    \n",
        "    print(\"System prompt token count:\",prompt_tokens)\n",
        "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
        "    print(\"Combined docs (context) token count:\",context_tokens)\n",
        "    print(\"--------\")\n",
        "    print(\"Requested token count:\",requested_tokens)\n",
        "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
        "    print(\"Chain Type selected:\", chain_type)\n",
        "        \n",
        "else:\n",
        "    print(\"NO RESULTS FROM AZURE SEARCH\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "System prompt token count: 1669\nMax Completion Token count: 1000\nCombined docs (context) token count: 628\n--------\nRequested token count: 3297\nToken limit for gpt-35-turbo : 4096\nChain Type selected: stuff\n"
        }
      ],
      "execution_count": 18,
      "metadata": {},
      "id": "880885fe-16bd-44bb-9556-7cb3d4989993"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use our Utility Chain from LangChain `qa_with_sources`"
      ],
      "metadata": {},
      "id": "1e232424-c7ba-4153-b23b-fb1fa2ebc64b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries  \n",
        "from azure.core.credentials import AzureKeyCredential  \n",
        "from azure.search.documents import SearchClient  \n",
        "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
        "from azure.search.documents.models import (\n",
        "    QueryAnswerType,\n",
        "    QueryCaptionType,\n",
        "    QueryLanguage,\n",
        "    QueryType,\n",
        "    RawVectorQuery,\n",
        "    VectorizableTextQuery,\n",
        "    VectorFilterMode,    \n",
        ")\n",
        "from azure.search.documents.indexes.models import (  \n",
        "    AzureOpenAIEmbeddingSkill,  \n",
        "    AzureOpenAIParameters,  \n",
        "    AzureOpenAIVectorizer,  \n",
        "    ExhaustiveKnnParameters,  \n",
        "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
        "    FieldMapping,  \n",
        "    HnswParameters,  \n",
        "    HnswVectorSearchAlgorithmConfiguration,  \n",
        "    IndexProjectionMode,  \n",
        "    InputFieldMappingEntry,  \n",
        "    OutputFieldMappingEntry,  \n",
        "    PrioritizedFields,    \n",
        "    SearchField,  \n",
        "    SearchFieldDataType,  \n",
        "    SearchIndex,  \n",
        "    SearchIndexer,  \n",
        "    SearchIndexerDataContainer,  \n",
        "    SearchIndexerDataSourceConnection,  \n",
        "    SearchIndexerIndexProjectionSelector,  \n",
        "    SearchIndexerIndexProjections,  \n",
        "    SearchIndexerIndexProjectionsParameters,  \n",
        "    SearchIndexerSkillset,  \n",
        "    SemanticConfiguration,  \n",
        "    SemanticField,  \n",
        "    SemanticSettings,  \n",
        "    SplitSkill,  \n",
        "    VectorSearch,  \n",
        "    VectorSearchAlgorithmKind,  \n",
        "    VectorSearchAlgorithmMetric,  \n",
        "    VectorSearchProfile,  \n",
        ")  \n",
        "\n",
        "from azure.storage.blob import BlobServiceClient  \n",
        "import openai  \n",
        "\n",
        "from dotenv import load_dotenv  \n",
        "import os  \n",
        "  \n",
        "# Configure environment variables  \n",
        "load_dotenv(\"credentials.env\")\n",
        "service_endpoint =os.getenv(\"AZURE_SEARCH_ENDPOINT\")  #os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
        "index_name =\"cogsrch-index-files\" # os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
        "key = os.getenv(\"AZURE_SEARCH_KEY\")  #remose _ADMIN_ here\n",
        "openai.api_type = \"azure\"  \n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
        "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
        "model: str = \"text-embedding-ada-002\"  \n",
        "blob_connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")  \n",
        "container_name =\"demo-vbd-mercedes\" # os.getenv(\"BLOB_CONTAINER_NAME\")  \n",
        "credential = AzureKeyCredential(key)  "
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701871102329
        }
      },
      "id": "a4e95dcf-f1d7-4394-8228-eff228a980de"
    },
    {
      "cell_type": "code",
      "source": [
        "# this functions needs to be adapted for calling openai in this notebook\n",
        "def get_search_results(query: str, indexes: list, \n",
        "                       k: int = 5,\n",
        "                       reranker_threshold: int = 1,\n",
        "                       sas_token: str = \"\",\n",
        "                       vector_search: bool = False,\n",
        "                       similarity_k: int = 3, \n",
        "                       query_vector: list = []) -> List[dict]:\n",
        "    \n",
        "    headers = {'Content-Type': 'application/json','api-key': os.environ[\"AZURE_SEARCH_KEY\"]}\n",
        "    params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}\n",
        "\n",
        "    agg_search_results = dict()\n",
        "    \n",
        "    for index in indexes:\n",
        "        search_payload = {\n",
        "            \"search\": query,\n",
        "            \"queryType\": \"semantic\",\n",
        "            \"semanticConfiguration\": \"my-semantic-config\",\n",
        "            \"count\": \"true\",\n",
        "            \"speller\": \"lexicon\",\n",
        "            \"queryLanguage\": \"en-us\",\n",
        "            \"captions\": \"extractive\",\n",
        "            \"answers\": \"extractive\",\n",
        "            \"top\": k\n",
        "        }\n",
        "        if vector_search:\n",
        "            search_payload[\"vectors\"]= [{\"value\": query_vector, \"fields\": \"chunkVector\",\"k\": k}]\n",
        "            search_payload[\"select\"]= \"id, title, chunk, name, location\"\n",
        "        else:\n",
        "            search_payload[\"select\"]= \"id, title, chunks, language, name, location, vectorized\"\n",
        "        \n",
        "\n",
        "        resp = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index + \"/docs/search\",\n",
        "                         data=json.dumps(search_payload), headers=headers, params=params)\n",
        "\n",
        "        search_results = resp.json()\n",
        "        agg_search_results[index] = search_results\n",
        "    \n",
        "    content = dict()\n",
        "    ordered_content = OrderedDict()\n",
        "    \n",
        "    for index,search_results in agg_search_results.items():\n",
        "        for result in search_results['value']:\n",
        "            if result['@search.rerankerScore'] > reranker_threshold: # Show results that are at least N% of the max possible score=4\n",
        "                content[result['id']]={\n",
        "                                        \"title\": result['title'], \n",
        "                                        \"name\": result['name'], \n",
        "                                        \"location\": result['location'] + sas_token if result['location'] else \"\",\n",
        "                                        \"caption\": result['@search.captions'][0]['text'],\n",
        "                                        \"index\": index\n",
        "                                    }\n",
        "                if vector_search:\n",
        "                    content[result['id']][\"chunk\"]= result['chunk']\n",
        "                    content[result['id']][\"score\"]= result['@search.score'] # Uses the Hybrid RRF score\n",
        "              \n",
        "                else:\n",
        "                    content[result['id']][\"chunks\"]= result['chunks']\n",
        "                    content[result['id']][\"language\"]= result['language']\n",
        "                    content[result['id']][\"score\"]= result['@search.rerankerScore'] # Uses the reranker score\n",
        "                    content[result['id']][\"vectorized\"]= result['vectorized']\n",
        "                \n",
        "    # After results have been filtered, sort and add the top k to the ordered_content\n",
        "    if vector_search:\n",
        "        topk = similarity_k\n",
        "    else:\n",
        "        topk = k*len(indexes)\n",
        "        \n",
        "    count = 0  # To keep track of the number of results added\n",
        "    for id in sorted(content, key=lambda x: content[x][\"score\"], reverse=True):\n",
        "        ordered_content[id] = content[id]\n",
        "        count += 1\n",
        "        if count >= topk:  # Stop after adding 5 results\n",
        "            break\n",
        "\n",
        "    return ordered_content"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b2074656-d2f9-4b22-a555-b7ed2ca5a908"
    },
    {
      "cell_type": "code",
      "source": [
        "vector_indexes = \"cogsrch-index-files\"\n",
        "\n",
        "k = 10\n",
        "similarity_k = 3\n",
        "ordered_results = get_search_results(QUESTION, vector_indexes,\n",
        "                                        k=k, # Number of results per vector index\n",
        "                                        reranker_threshold=1,\n",
        "                                        vector_search=True, \n",
        "                                        similarity_k=similarity_k,\n",
        "                                        query_vector = embedder.embed_query(QUESTION)\n",
        "                                        )\n",
        "print(\"Number of results:\",len(ordered_results))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'value'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m similarity_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ordered_results \u001b[38;5;241m=\u001b[39m \u001b[43mget_search_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUESTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of results per vector index\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mreranker_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mvector_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msimilarity_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUESTION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of results:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(ordered_results))\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/eweigel3/code/Users/eweigel/Azure-Cognitive-Search-Azure-OpenAI-Accelerator/common/utils.py:309\u001b[0m, in \u001b[0;36mget_search_results\u001b[0;34m(query, indexes, k, reranker_threshold, sas_token, vector_search, similarity_k, query_vector)\u001b[0m\n\u001b[1;32m    306\u001b[0m ordered_content \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,search_results \u001b[38;5;129;01min\u001b[39;00m agg_search_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43msearch_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@search.rerankerScore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m reranker_threshold: \u001b[38;5;66;03m# Show results that are at least N% of the max possible score=4\u001b[39;00m\n\u001b[1;32m    311\u001b[0m             content[result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    312\u001b[0m                                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    313\u001b[0m                                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index\n\u001b[1;32m    317\u001b[0m                                 }\n",
            "\u001b[0;31mKeyError\u001b[0m: 'value'"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701877640119
        }
      },
      "id": "67e88f59-993f-48b8-bc29-c7aaef7f58a2"
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"cogsrch-index-files\"\n",
        "# Pure Vector Search\n",
        "query = \"summarize Door Locks and Door Retention Components\"  \n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "vector_query = VectorizableTextQuery(text=query, k=10, fields=\"vector\", exhaustive=True)\n",
        "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
        "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k=3, fields=\"vector\")\n",
        "  \n",
        "results = search_client.search(  \n",
        "    search_text=None,  \n",
        "    vector_queries= [vector_query],\n",
        "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
        "    #filter=\"filter eq 'id1' \",\n",
        "    #filter=\"filter/any(filter: search.in(filter, 'group_id1, group_id2'))\"  ,\n",
        "    top=10\n",
        ")  \n",
        "  \n",
        "for result in results:  \n",
        "    print(f\"parent_id: {result['parent_id']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    #print(f\"Content: {result['chunk']}\")   \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "parent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_3\nScore: 0.8714593\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_25\nScore: 0.8665724\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_4\nScore: 0.856793\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_12\nScore: 0.85532546\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_2\nScore: 0.85347956\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_11\nScore: 0.8501605\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_24\nScore: 0.84761506\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_5\nScore: 0.843995\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_21\nScore: 0.8416074\nparent_id: aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2\nchunk_id: 7f98ad2134ee_aHR0cHM6Ly9zdG9yYWdlZGVtb29wZW5haS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGVtby12YmQtbWVyY2VkZXMvMjYwNTI0MTYzOSUyMFtPcmlnaW5hbCUyMGVuXSUyMENBTiUyMFRTRCUyMDIwNiUyMFJldiUyMDIlMjBFTiUyMDIwMTAtMDYtMDMucGRmLnBkZg2_pages_6\nScore: 0.8393594\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701874611477
        }
      },
      "id": "dbcdc420-aac7-4537-8e3a-92589c086e81"
    },
    {
      "cell_type": "code",
      "source": [
        "chain_type =  \"stuff\"  "
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701873803755
        }
      },
      "id": "6251112c-e9bb-4622-a4da-233979816882"
    },
    {
      "cell_type": "code",
      "source": [
        "if chain_type == \"stuff\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       prompt=COMBINE_PROMPT)\n",
        "elif chain_type == \"map_reduce\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
        "                                       combine_prompt=COMBINE_PROMPT,\n",
        "                                       return_intermediate_steps=True)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1701873876509
        }
      },
      "id": "511273b3-256d-4e60-be72-ccd4a74cb885"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "QUESTION = \"summarize Door Locks and Door Retention Components\"\n",
        "# Try with other language as well\n",
        "response = chain({\"input_documents\": results, \"question\": query, \"language\": \"English\"})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 48.9 ms, sys: 26 µs, total: 48.9 ms\nWall time: 1.95 s\n"
        }
      ],
      "execution_count": 37,
      "metadata": {},
      "id": "b99a0c19-d48c-41e9-8d6c-6d9f13d29da3"
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<iterator object azure.core.paging.ItemPaged at 0x7f5d20a8cf10>\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701877527616
        }
      },
      "id": "1444605d-2fe7-4c4b-82d8-d3b3250def56"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['output_text']))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "I'm sorry, but I can't provide the information you're looking for because there are no extracted parts provided for me to reference."
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1701874620509
        }
      },
      "id": "37f7fa67-f67b-402e-89e3-266d5d6d21d8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please Note**: There are some instances where, despite the answer's high accuracy and quality, the references are not done according to the instructions provided in the COMBINE_PROMPT. This behavior is anticipated when dealing with GPT-3.5 models. We will provide a more detailed explanation of this phenomenon towards the conclusion of Notebook 5."
      ],
      "metadata": {},
      "id": "05e27c75-bfd9-4304-b2fd-c8e30bcc0558"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if you want to inspect the results from map_reduce chain type, each top similar chunk summary (k=4 by default)\n",
        "\n",
        "# if chain_type == \"map_reduce\":\n",
        "#     for step in response['intermediate_steps']:\n",
        "#         display(HTML(\"<b>Chunk Summary:</b> \" + step))"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {},
      "id": "11345374-6420-4b36-b061-795d2a804c85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
        "- Utilizing Azure Cognitive Search, we conduct a multi-index text-based search that identifies the top documents from each index.\n",
        "- Utilizing Azure Cognitive Search's vector search, we extract the most relevant chunks of information.\n",
        "- Subsequently, Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.\n",
        "- Best of two worlds!"
      ],
      "metadata": {},
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
      ],
      "metadata": {},
      "id": "fdc6e2fe-1c34-4952-99ad-14940f022379"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}